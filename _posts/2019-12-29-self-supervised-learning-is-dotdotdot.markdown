---
mathjax: true
layout:  post
title:   "Self-Supervised Learning is, Omnipresent/Cake GÃ©noise/?"
date:    2019-12-29 01:16:00 -0800
---
From PixelRNN, PixelCNN, WaveNets, to Language Models, to Self-Play, Hindsight Experience Replay, Self-Supervised Learning(SSL) is everywhere.

ELMO: deep contextualized
Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.

Using in- trinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used with- out modification to perform well on supervised word sense disambiguation tasks) while lower- level states model aspects of syntax (e.g., they can be used to do part-of-speech tagging).

ELMo is a task specific combination of the in- termediate layer representations in the biLM.

More direct context embeddings
helpful using pre-training



model architectures
pre-training
how pretrained and how used
training objective
application
what gain?


[van den Oord et al. 2019][Representation Learning with Contrastive Predictive Coding]
[He et al. 2020][Momentum Contrast for Unsupervised Visual Representation Learning]
[Chen et al. 2020][A Simple Framework for Contrastive Learning of Visual Representations]

[Representation Learning with Contrastive Predictive Coding]: https://arxiv.org/pdf/1807.03748.pdf
[Momentum Contrast for Unsupervised Visual Representation Learning]: https://arxiv.org/pdf/1911.05722.pdf
[A Simple Framework for Contrastive Learning of Visual Representations]: https://arxiv.org/pdf/2002.05709.pdf
