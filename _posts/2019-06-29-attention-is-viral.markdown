---
mathjax: true
layout:  post
title:   "Attention is Viral!"
date:    2019-06-29 11:13:18 -0800
---
Attention is a big idea in DL. It mitigates a pain point in RNN/GRU/LSTM but is not just about that.

In the RNN/GRU/LSTM Encoder-Decoder framework ([Cho et al. 2014][Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation], [Sutskever et al. 2014][Sequence to Sequence Learning with Neural Networks]), an input sequence is encoded as a context vector $\mathbf{c}$ after one pass of the sequence, then conditional on $\mathbf{c}$, the decoder generates an output sequence. The pain point is that the context vector is inflexible to abstract information effectively. When input sequence is long, quality degrades. Theoretically GRU/LSTM mitigates the problem but practically they still come short.

Here comes the attention mechanism. I will talk about attention [Bahdanau et al. 2014][Neural Machine Translation by Jointly Learning to Align and Translate], self-attention [Cheng et al. 2016][Long Short-Term Memory-Networks for Machine Reading], and the generalized self-attention [Vaswani et al. 2017][Attention is All You Need]. All can be read in a key, value, query aspect.

Attention mechanism is built on top of the Encoder-Decoder framework. In Encoder, the input is encoded into a collection of key, value pairs. The interpretation is that each key, value pair annotates a part of the input. For instance in [Bahdanau et al. 2014][Neural Machine Translation by Jointly Learning to Align and Translate] the input is a sequence $\mathbf{x} = (x_1, ..., x_{T_\mathbf{x}})$, and each word $x_j$ is annotated as $h_j=[\overrightarrow{h_j}^\top; \overleftarrow{h_j}^\top]^\top$ using Bi-RNN. We can also use Bi-GRU, Bi-LSTM, or anything. This is the Encoder. In Decoder, using the above example, traditionally the current word $y_i$ is generated by

$$ s_i = f(s_{i-1}, y_{i-1}, \mathbf{c}), $$

$$ p(y_i|y_{i-1}, ..., y_1, \mathbf{c}) = g(s_i, y_{i-1}, \mathbf{c}), $$

where $s_i$ is the RNN hidden state and $y_{i-1}$ is the previously generated word. The inventive part in [Bahdanau et al. 2014][Neural Machine Translation by Jointly Learning to Align and Translate] is that the context vector $\mathbf{c}$ is reborn and becomes adaptive. Specifically, now $y_i$ is generated by

$$s_i = f(s_{i-1}, y_{i-1}, c_i),$$

$$p(y_i|y_{i-1}, ..., y_1, \mathbf{x}) = g(s_i, y_{i-1}, c_i).$$

The context vector $c_i$ can adaptively attend to, or align to the entire input sequence $\mathbf{x} = (x_1, ..., x_{T_\mathbf{x}})$. The way $c_i$ is obtained is called the attention function. Attention bridges Decoder and Encoder. In the paper, $c_i$ is a weighted average

$$c_i = \sum_{j=1}^{T_\mathbf{x}}\alpha_{ij}h_j,$$

and the weights are

$$(\alpha_{ij}|j=1, ..., T_{\mathbf{x}}) = softmax((a(s_{i-1}, h_j)|j=1, ..., T_{\mathbf{x}})).$$

We can think of $s_{i-1}$ as the query hitting on each key, value pair $(h_j, h_j), j=1, ..., T_{\mathbf{x}}$. Encoder, Decoder, and attention together specify the model. Andrew Ng's deeplearning.ai [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) course week 3 has a notebook with code example.

Let's change gear to talk about self-attention. Unlike seq2seq, self-attention involves only 1 sequence. Originally in [Cheng et al. 2016][Long Short-Term Memory-Networks for Machine Reading] self-attention was introduced as an improved version of LSTM, by introducing attention of the current word on *previous* words and introducing the current hidden state context vector and the current memory context vector, and then using these context vectors in the traditional LSTM. In this approach, self-attention like LSTM requires one pass of the sequence. Later in [Vaswani et al. 2017][Attention is All You Need], self-attention breaks away from the sequential setting and simply denotes re-representation, similar to what a convolution layer does. In the paper, the key, value, query are all linear projections of the previous representation. The generalized self-attention is parallelizable. We can combine attention and self-attention to get a better Encoder-Decoder framework.

[Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation]: https://arxiv.org/pdf/1406.1078
[Sequence to Sequence Learning with Neural Networks]: https://arxiv.org/pdf/1409.3215.pdf
[Neural Machine Translation by Jointly Learning to Align and Translate]: https://arxiv.org/pdf/1409.0473.pdf
[Long Short-Term Memory-Networks for Machine Reading]: https://arxiv.org/pdf/1601.06733.pdf
[Attention is All You Need]: https://arxiv.org/pdf/1706.03762.pdf
