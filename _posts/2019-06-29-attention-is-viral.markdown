---
mathjax: true
layout:  post
title:   "Attention is Viral!"
date:    2019-06-29 11:13:18 -0800
---
*In a Hilbert space, the linear span of a basis is dense.*

Attention is a big idea in DL. It mitigates a pain point in RNN/GRU/LSTM but is not just about that.

In the RNN/GRU/LSTM Encoder-Decoder framework ([Cho et al. 2014][Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation], [Sutskever et al. 2014][Sequence to Sequence Learning with Neural Networks]), an input sequence is encoded as a context vector $\mathbf{c}$ after one pass of the sequence, then conditional on $\mathbf{c}$, the decoder generates an output sequence. The pain point is that the context vector is inflexible to abstract information effectively. When input sequence is long, quality degrades. Theoretically GRU/LSTM mitigates the problem but practically they still come short.

Here comes the attention mechanism. I will talk about attention [Bahdanau et al. 2014][Neural Machine Translation by Jointly Learning to Align and Translate], self-attention [Cheng et al. 2016][Long Short-Term Memory-Networks for Machine Reading], generalized attention and self-attention (all can be read in a query, key, value aspect), and the Transformer architecture [Vaswani et al. 2017][Attention is All You Need].

Attention mechanism is built on top of the Encoder-Decoder framework. It is like information retrieval. In Encoder, the input is encoded into a collection of key, value pairs. In general, temporal/spatial/etc. information can be added to those pairs. The interpretation is that each key, value pair annotates a part of the input. For instance in [Bahdanau et al. 2014][Neural Machine Translation by Jointly Learning to Align and Translate] the input is a sequence $\mathbf{x} = (x_1, ..., x_{T_\mathbf{x}})$, and each word $x_j$ is annotated as $h_j=[\overrightarrow{h_j}^\top; \overleftarrow{h_j}^\top]^\top$ using Bi-RNN. We can also use Bi-GRU, Bi-LSTM, or anything. This is the Encoder. In Decoder, the retriever, using the above example, traditionally the current word $y_i$ is generated by

$$s_i = f(s_{i-1}, y_{i-1}, \mathbf{c}),$$

$$p(y_i|y_{i-1}, ..., y_1, \mathbf{c}) = g(s_i, y_{i-1}, \mathbf{c}),$$

where $s_i$ is the RNN hidden state and $y_{i-1}$ is the previously generated word. The inventive part in [Bahdanau et al. 2014][Neural Machine Translation by Jointly Learning to Align and Translate] is that the context vector $\mathbf{c}$ is reborn and becomes adaptive. Specifically, now $y_i$ is generated by

$$s_i = f(s_{i-1}, y_{i-1}, c_i),$$

$$p(y_i|y_{i-1}, ..., y_1, \mathbf{x}) = g(s_i, y_{i-1}, c_i).$$

The context vector $c_i$ can adaptively attend to, or align to the entire input sequence $\mathbf{x} = (x_1, ..., x_{T_\mathbf{x}})$. The way $c_i$ is obtained is called the attention function. Attention bridges the Decoder and Encoder. In the paper, $c_i$ is a weighted average

$$c_i = \sum_{j=1}^{T_\mathbf{x}}\alpha_{ij}h_j,$$

and the weights are

$$(\alpha_{ij}|j=1, ..., T_{\mathbf{x}}) = softmax((a(s_{i-1}, h_j)|j=1, ..., T_{\mathbf{x}})).$$

We can think of $s_{i-1}$ as the query hitting on each key, value pair $(h_j, h_j), j=1, ..., T_{\mathbf{x}}$. Mathematically, there is a representation part of query $\rightarrow$ keys and a morphism part of keys $\rightarrow$ values. The Encoder, Decoder, and the attention mechanism together specify the model.

Let's change gear to talk about self-attention. Unlike seq2seq, self-attention involves only 1 sequence. Originally in [Cheng et al. 2016][Long Short-Term Memory-Networks for Machine Reading], self-attention was introduced to improve LSTM. It was still LSTM where the current word attended to *previous* words. It introduced context vectors for each hidden state and each memory, and then used them in the traditional LSTM. In this approach, self-attention like LSTM, requires one pass of the sequence.

Later in [Vaswani et al. 2017][Attention is All You Need], self-attention was vastly reengineered. It broke away from the sequential setting and diverged from RNN/GRU/LSTM. It is more like CNN but is more flexible and probably better. It is re-representation. It is self information retrieval. The core is a representation part of query $\rightarrow$ keys and a morphism part of keys $\rightarrow$ values. Key, value pairs can be infinite. In the paper, the authors used multi-head attention. They also encoded temporal information with positional encoding. Since keys have order, mask can apply. The generalized self-attention is parallelizable. The authors also introduced the Transformer architecture in the paper. Transformer has a few good designs besides attention.

[Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation]: https://arxiv.org/pdf/1406.1078
[Sequence to Sequence Learning with Neural Networks]: https://arxiv.org/pdf/1409.3215.pdf
[Neural Machine Translation by Jointly Learning to Align and Translate]: https://arxiv.org/pdf/1409.0473.pdf
[Long Short-Term Memory-Networks for Machine Reading]: https://arxiv.org/pdf/1601.06733.pdf
[Attention is All You Need]: https://arxiv.org/pdf/1706.03762.pdf
