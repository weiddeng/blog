---
mathjax: true
layout:  post
title:   "WaveNet and its Friends"
date:    2020-05-26 08:31:00 -0800
---

[Osindero et al. 2008][Modeling Image Patches with a Directed Hierarchy of Markov Random Fields]: divide(using Deep Belief Nets)-and-conquer(conditional Markov Random Fields) for generating high dimensional data.  
[Larochelle et al. 2011][The Neural Autoregressive Distribution Estimator]: Neural Autoregressive Distribution Estimator (NADE), modeling the distribution of high dimensional data, autoregressive, using DBN to tractably replace restricted Boltzmann machine (RBM), similar to [Osindero et al. 2008][Modeling Image Patches with a Directed Hierarchy of Markov Random Fields].  
[Theis et al. 2015][Generative Image Modeling Using Spatial LSTMs]: Recurrent Image Density Estimator (RIDE). Combine two ideas: mixtures of conditional Gaussian scale mixtures (MCGSMs) and spatial LSTM (SLSTM). SLSTM is a special case of the multi-dimensional LSTM where each memory unit has two preceding states $c_{i,j−1}$ and $c_{i−1,j}$, and two corresponding forget gates $f_{i,j}^r$ and $f_{i,j}^c$. Sequentially read relatively small neighborhoods of pixels from the image and produce a hidden vector at every pixel. Then feed the hidden vector to a factorized MCGSM to predict the state of the corresponding pixel, $p(x_{i,j} \mid x_{<ij}) = p(x_{i,j} \mid h_{ij})$.  
[Gregor et al. 2015][DRAW: A Recurrent Neural Network For Image Generation]: DRAW has the following ideas: 1. generate image iteratively rather than in a single pass (similar to boosting?) 2. autoregressive variational autoencoder architecture 3. use a dynamically updated attention mechanism to restrict both the input region observed by the encoder and the output region modified by the decoder. MNIST and Street View House Number Generations are pretty good. Generating CIFAR images is moderate.  
[van den Oord et al. 2016][Pixel Recurrent Neural Networks]: See [Lebanoff 2017][PixelCNN, PixelRNN Youtube] for a good description of PixelCNN and PixelRNN. PixelCNN uses feature maps and PixelRNN uses hidden stats and has input-to-state and state-to-state maps.  
[van den Oord et al. 2016][Conditional Image Generation with PixelCNN Decoders]: introducing Gated PixelCNN and Conditional Gated PixelCNN, (gated convolution layer, residual connection - used in previous papers too), with two stacks of CNNs to eliminate blind spots in the receptive field, similar to RowLSTM(?). It is an improvement over the original PixelCNN that is able to match or outperform PixelRNN. CIFAR-10 generation is pretty good. Conditional image generation is also pretty cool.  
[van den Oord et al. 2016][Wavenet: A Generative Model For Raw Audio]: dilated convolution, text-to-speech, speech recognition,


[Modeling Image Patches with a Directed Hierarchy of Markov Random Fields]: https://papers.nips.cc/paper/3279-modeling-image-patches-with-a-directed-hierarchy-of-markov-random-fields.pdf
[The Neural Autoregressive Distribution Estimator]: http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf
[Generative Image Modeling Using Spatial LSTMs]: https://arxiv.org/pdf/1506.03478.pdf
[DRAW: A Recurrent Neural Network For Image Generation]: https://arxiv.org/pdf/1502.04623.pdf
[Pixel Recurrent Neural Networks]: https://arxiv.org/pdf/1601.06759.pdf
[PixelCNN, PixelRNN Youtube]: https://www.youtube.com/watch?v=-FFveGrG46w
[Conditional Image Generation with PixelCNN Decoders]: https://arxiv.org/pdf/1606.05328.pdf
[Generating Images from Captions with Attention]: https://arxiv.org/pdf/1511.02793.pdf
[Wavenet: A Generative Model For Raw Audio]: https://arxiv.org/pdf/1609.03499.pdf
