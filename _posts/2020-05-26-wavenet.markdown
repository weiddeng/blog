---
mathjax: true
layout:  post
title:   "WaveNet and its Friends"
date:    2020-05-26 08:31:00 -0800
---

[Osindero et al. 2008][Modeling Image Patches with a Directed Hierarchy of Markov Random Fields]: use Deep Belief Nets to stack a directed hierarchy of conditional Markov Random Fields, energy-based learning, not exactly following the negative log likelihood gradient, layer-by-layer learning, sample a hidden vector $h$ from the conditional posterior $p(h \mid v)$ in the lower-level restricted Boltzmann machine and feed it as the input data to the higher-level RBM that learns the next layer of features. After learning, data can be sampled top-down all the way to the input data layer to generate input images. The generation process uses MCMC and is intractable.  
[Larochelle et al. 2011][The Neural Autoregressive Distribution Estimator]: Neural Autoregressive Distribution Estimator (NADE), model the distribution of high dimensional data tractably, decompose the joint distribution into conditional distributions,
$$
\begin{align}
p(v)&=\prod_{i=1}^D p(v_i \mid (v_1, ..., v_{i-1}))\\
&=\prod_{i=1}^D \frac{p(v_1, ..., v_i)}{p(v_1, ..., v_{i-1})}\\
&=\prod_{i=1}^D \frac{\sum\limits_{(w_{i+1}, ..., w_D)}p(v_1, ..., v_i, w_{i+1}, ..., w_D)}{\sum\limits_{(w_i, ..., w_D)}p(v_1, ..., v_{i-1}, w_i, ..., w_D)}\\
&=\prod_{i=1}^D \frac{\sum\limits_{(w_{i+1}, ..., w_D)} \sum\limits_h \exp(-E((v_1, ..., v_i, w_{i+1}, ..., w_D), h))} {\sum\limits_{(w_i, ..., w_D)}\sum\limits_h \exp(-E((v_1, ..., v_{i-1}, w_i, ..., w_D), h))}
\end{align}
$$


autoregressive, DBN, MLE.  
[Theis et al. 2015][Generative Image Modeling Using Spatial LSTMs]: Recurrent Image Density Estimator (RIDE). Combine two ideas: mixtures of conditional Gaussian scale mixtures (MCGSMs) and spatial LSTM (SLSTM). SLSTM is a special case of the multi-dimensional LSTM where each memory unit has two preceding states, $c_{i,j−1}$ and $c_{i−1,j}$, and two forget gates, $f_{i,j}^r$ and $f_{i,j}^c$. Sequentially read neighborhoods of pixels from image to produce a hidden vector at every pixel, $p(x_{i,j} \mid x_{<ij}) = p(x_{i,j} \mid h_{ij})$, MLE.  
[Gregor et al. 2015][DRAW: A Recurrent Neural Network For Image Generation]: DRAW has the following ideas: 1. generate image iteratively rather than in a single pass (similar to boosting?) 2. autoregressive variational autoencoder architecture 3. use a dynamically updated attention mechanism to restrict both the input region observed by the encoder and the output region modified by the decoder. MNIST and Street View House Number generations are pretty good. CIFAR-10 generation is moderate.
<center><img src="../assets/draw.png" width="450"/></center>
<br />

[van den Oord et al. 2016][Pixel Recurrent Neural Networks]: See [Lebanoff 2017][PixelCNN, PixelRNN Youtube] for a good description of PixelCNN and PixelRNN. PixelCNN uses feature maps and PixelRNN uses hidden states. Convolutional LSTM, Row LSTM, etc.
<center><img src="../assets/pixelcnnrnn.png" width="450"/></center>
<br />

[van den Oord et al. 2016][Conditional Image Generation with PixelCNN Decoders]: Gated PixelCNN and Conditional Gated PixelCNN (gated convolution layer, residual connection - used in previous papers too), with two stacks of CNNs to eliminate blind spots in the receptive field, in order to get the best of both PixelCNN and PixelRNN. CIFAR-10 generation is pretty good. UnConditional to Conditional, training data are pairs: (conditional vector, image), intelligently blend in the conditional vector.  
[van den Oord et al. 2016][Wavenet: A Generative Model For Raw Audio]: dilated convolution to obtain a large receptive field:
<center><img src="../assets/wavenet.png" width="450"/></center>
<br />
test-to-speech: transform the text into a sequence of linguistic and phonetic features (which contain information about the current phoneme, syllable, word, etc.) and feed it to WaveNet, the network’s predictions are conditioned not only on the previous audio samples, but also on the text, the result is kind babbling though.

Reference:  
[Modeling Image Patches with a Directed Hierarchy of Markov Random Fields][Modeling Image Patches with a Directed Hierarchy of Markov Random Fields]  
[The Neural Autoregressive Distribution Estimator][The Neural Autoregressive Distribution Estimator]  
[Generative Image Modeling Using Spatial LSTMs][Generative Image Modeling Using Spatial LSTMs]  
[DRAW: A Recurrent Neural Network For Image Generation][DRAW: A Recurrent Neural Network For Image Generation]  
[Pixel Recurrent Neural Networks][Pixel Recurrent Neural Networks]  
[PixelCNN, PixelRNN Youtube][PixelCNN, PixelRNN Youtube]  
[Conditional Image Generation with PixelCNN Decoders][Conditional Image Generation with PixelCNN Decoders]  
[Generating Images from Captions with Attention][Generating Images from Captions with Attention]  
[Wavenet: A Generative Model For Raw Audio][Wavenet: A Generative Model For Raw Audio]  

[Modeling Image Patches with a Directed Hierarchy of Markov Random Fields]: https://papers.nips.cc/paper/3279-modeling-image-patches-with-a-directed-hierarchy-of-markov-random-fields.pdf
[The Neural Autoregressive Distribution Estimator]: http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf
[Generative Image Modeling Using Spatial LSTMs]: https://arxiv.org/pdf/1506.03478.pdf
[DRAW: A Recurrent Neural Network For Image Generation]: https://arxiv.org/pdf/1502.04623.pdf
[Pixel Recurrent Neural Networks]: https://arxiv.org/pdf/1601.06759.pdf
[PixelCNN, PixelRNN Youtube]: https://www.youtube.com/watch?v=-FFveGrG46w
[Conditional Image Generation with PixelCNN Decoders]: https://arxiv.org/pdf/1606.05328.pdf
[Generating Images from Captions with Attention]: https://arxiv.org/pdf/1511.02793.pdf
[Wavenet: A Generative Model For Raw Audio]: https://arxiv.org/pdf/1609.03499.pdf
