---
mathjax: true
layout:  post
title:   "Chatbot - Partly Attention, Partly Evolution"
date:    2020-08-09 14:10:00 -0800
---

Chatbot is a vast topic, so vast that it probably stirs up a hornets' nest. A chatbot doesn't bake meal or fold laundry, and isn't just about information retrieval or question answering. However, I think language which includes part of communication is a big part of human intelligence.

This post will try to discuss a range of topics.

### Tokenization

Tokenization is one of the first steps in an NLP project, and it dives right into the wonderful world of preprocessing. This is where ML meets domain knowledge. For simplicity, think of a sentence as a unit. How to tokenize a sentence? Usually we want to possess a wide vocabulary, and fragment the sentence to an ensemble of tokens in the vocab. The fragmentation step is a geometric mapping. We can think a sentence is a line or a closed interval, and a fragmentation is a path in the vocab space. So how to build a wide vocab? 1. Explore vs. Exploit; 2. Define the vocab space or tokens thoughtfully. Preprocessing is ineluctable. It is arbitrary and can have a big impact, but it offers an opportunity to adapt to an application. A vocab is kind of static but it will evolve. An ideal scenario is whenever we draw a sentence, and fragment or tokenize it, we can retrieve rich information from the vocab for every token. In particular, it would be great if we have seen every token before and seen it many times. The [Byte Pair Encoding a.k.a. BPE](https://arxiv.org/pdf/1508.07909.pdf) adopted this strategy.

#### BPE

BPE breaks down a corpus into words, and then breaks down each word into bytes. If characters are unambiguous, then instead it breaks down each word into characters. WLOG let's use characters. The last character of each word is concatenated with an end-of-word symbol $$\cdot$$ and the concatenated character plus $$\cdot$$ is treated as one token. This has the advantage of differentiating end-of-word and making it unambiguous. For example, the `ter` in `term` and `bitter` are different. A second example, in sentence generation, it doesn't rely on sampling the end-of-word token to claim the end of a word. Initially the token vocab is the "character" vocab - note a "character" is a pure character or a character concatenated with $$\cdot$$. Then bottom-up merges are performed on consecutive token pairs prioritized by frequency. Each merge produces a new token. The final token vocab size is the size of the initial vocab plus the number of merges â€“ the latter is the only hyperparameter. BPE was originally introduced for translation tasks, and the authors tried learning the encoding with the source and target vocabs combined which they called joint BPE. They also tried dropping infrequent subword units to reduce noise. Applying BPE to encode a word is [pretty straightforward](http://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html). BPE is capable of encoding open vocabs by representing rare and unseen words as a sequence of subword units, but BPE still has a gap to semantics. For more, check out Hugging Face's [tokenizer summary](https://huggingface.co/transformers/tokenizer_summary.html).

### Training Data

When I started out, I was fixed on the "model". However, 1. There are *a lot of* "peripherals" *within* modeling. 2. As in September 2020, it seems to me that the model backbone is floored on the Transformer encoder-decoder seq2seq lottery. So the "details" ("peripherals") matter a lot.

PersonaChat dataset -->  ConvAI2

Training data is tied to task/skills/learning plan.

dialogues

Attention is all you need:- WMT 2014 English- to-German translation, WMT 2014 English-to-French translation.

data replication, inspiration retrieval or generated,

####Training Setup

sentence 2 sentence

####Training Evaluation

####Adaptation

####Generation Setup

####Generation Evaluation

####Learning Online