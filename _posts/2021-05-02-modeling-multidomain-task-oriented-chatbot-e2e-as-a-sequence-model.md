---
mathjax: true
layout:  post
title:   "Modeling Multidomain Task Oriented Chatbot E2E as a Sequence Model"
date:    2021-05-02 21:28:00 -0800
---

Task oriented chatbot is designed to assist users to finish tasks as efficiently as possible. Unlike open domain chatbot, we can target a handful of domains, and a fixed list of tasks or task templates. So we can set up the stage in the beginning.

With the domains, tasks, and applications in mind, it is time to collect and curate data. Multi-Domain Wizard-of-Oz dataset ([MultiWOZ](https://arxiv.org/abs/1810.00278)) shows an example. In MultiWOZ, there are 7 domains: restaurant, hotel, attraction, taxi, train, hospital, police. And a task/application looks like: You wanna book a train to Cambridge on Friday, book a hotel in Kendall Square, and look for attractions nearby. MultiWOZ uses human-to-human dialogues to collect data, as well as human to annotate every turn in the collected dialogues.

The human-to-human dialogues are collected in a Wizard-of-Oz fashion. The paper shows the interface from the wizard side and the user side. Wizard can access database to retrieve values and use them to fill or compose dialogues. On the annotation side, at every turn, a human annotator annotates the belief states after the user's utterance, and the actions after the wizard's utterance. A belief state is a list of tuples [(domain, slot name, value), ...] such as [(restaurant, price, moderate), ...]. An action is a list of tuples [(domain, act, slot name), ...] such as [(restaurant, request, location), ...]. Another example of such a tuple is (restaurant, offer to book, _). A task oriented dialogue system is often specified by an ontology, i.e., a structured representation of the back-end database. The ontology defines all entity attributes called slots and all possible values for each slot. Based on a given ontology spanning several domains, a task template is created for each task by random sampling. This results in single- and multi-domain dialogue scenarios and domain specific constraints are generated.

With the dialogues collected, at each turn $t$ of the wizard/system ($S$), all previous turns are treated as the context $$C_t = [U_0, S_0, ..., U_t]$$. Then the belief state is generated $$B_t = f(C_t)$$, which is used to query the database and get values $$D_t = Retrieve(B_t)$$. Then the action is generated $$A_t = f(C_t, B_t, D_t)$$. Finally the wizard's response is generated $$S_t = f(C_t, B_t, D_t, A_t)$$. A single training datapoint consists of the concatenation $$x^t = [C_t, B_t, D_t, A_t, S_t]$$. The [simpletod GitHub repo](https://github.com/salesforce/simpletod) shows a great example of fine tuning GPT2 on this custom data. This is transfer learning. The tokenization and embedding of a previous work is used. The training model architecture is a seq-2-seq transformer where the output sequence is the input sequence shifted to the right by 1 token.

[A Simple Language Model for Task-Oriented Dialogue](https://arxiv.org/abs/2005.00796)